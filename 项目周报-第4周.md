# 项目周报
日期：2025-4-26

项目实践题目：面向机器学习的数据清洗方法
### 实践内容
项目涉及到数据预处理和特征工程的学习，重点是如何清洗数据、探索数据集、识别数据中的问题，并使用相关工具来处理数据问题。
### 学习目标
- **学习使用Google Scholar来查找文献**
- **通过使用sklearn的官方数据集实现knn算法**
 
#### 使用Google Scholar来查找文献
- 对所给两篇论文的搜索结果：
![alt text](d9c4a94eaa8423f67937432f9e2031c.png)

- 使用Google Scholar进行文献搜索时，可以通过以下几种方式来查找文献：
  - 直接输入关键词：输入关键词后，Google Scholar会自动搜索与关键词相关的文献。
  - 高级搜索：Google Scholar提供了高级搜索功能，可以根据不同的搜索条件来查找文献。
  - 搜索结果过滤：Google Scholar提供了多种搜索结果过滤选项，可以根据不同的条件来过滤搜索结果。
  等等.......
###### 使用Google Scholar，体验颇为实用。它汇聚海量学术文献，搜索功能便捷，输入关键词就能获取诸多相关研究成果。查看引用文献功能方便梳理研究脉络，找综述或相关重要论文时很高效。对于学术探索来说，它是一个有价值的工具，能为研究提供丰富资料和思路。 
### 使用sklearn的官方数据集实现knn算法
#### sklearnknn算法中的一些参数介绍
- n_neighbors：k值，即选取最近的k个点
- weights：权重参数，默认是uniform，也可以选择distance，表示权重与距离成反比
- algorithm：搜索算法，默认是auto，可以选择ball_tree、kd_tree、brute
- leaf_size：如果是选择ball_tree或者kd_tree，可以设置叶子节点数量   
- metric：距离度量，默认是minkowski，可以选择其他距离度量方式，如欧氏距离、曼哈顿距离等
- p：距离度量公式的幂参数，当p=1时，为曼哈顿距离，p=2时，为欧氏距离
- metric_params：距离度量公式的其他关键参数
- n_jobs：并行任务数量
#### 通过sklearn中提供的交叉验证的方式来确定k的取值
- 相关代码如下
```python
import sklearn
from sklearn.datasets import load_iris
from sklearn.model_selection  import cross_val_score
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

#读取鸢尾花数据集
iris = load_iris()
x = iris.data
y = iris.target
k_range = range(1, 31)
k_error = []
#循环，取k=1到k=31，查看误差效果
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    #cv参数决定数据集划分比例，这里是按照5:1划分训练集和测试集
    scores = cross_val_score(knn, x, y, cv=6, scoring='accuracy')
    k_error.append(1 - scores.mean())

#画图，x轴为k值，y值为误差值
plt.plot(k_range, k_error)
plt.xlabel('Value of K for KNN')
plt.ylabel('Error')
plt.show()
```
- 在运行程序后可以获得如下图所示k不同取值时的方差

![alt text](a5d69ef018620e04f8b7b65935bca35.png)
- 可以看出当k=11时，方差最小，所以k取11时效果最好
有了K值就能运行KNN算法了，具体代码如下：
```python
import matplotlib.pyplot as plt
from numpy import *
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
import numpy as np
n_neighbors = 11

# 导入一些的数据
iris = datasets.load_iris()
x = iris.data[:, :2]  # 采用前两个feature,方便画图在二维平面显示
y = iris.target


h = .02  # 网格中的步长

# 创建彩色的图
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])


#weights是KNN模型中的一个参数，上述参数介绍中有介绍，这里绘制两种权重参数下KNN的效果图
for weights in ['uniform', 'distance']:
    # 创建了一个knn分类器的实例，并拟合数据。
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(x, y)

    # 绘制决策边界。为此，将为每个分配一个颜色
    # 来绘制网格中的点 [x_min, x_max]x[y_min, y_max].
    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # 将结果放入一个彩色图中
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # 绘制训练点
    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n_neighbors, weights))

plt.show()
```
- 运行后可以得到两种不同权重参数下的KNN算法的效果图

![alt text](da93052b6df2d8aa18c97429f876376.png) 
- 两者差异
决策边界：
左图（weights = 'uniform'）：由于每个邻居点权重相同，决策边界相对较为平滑和规则，不同类别区域的划分相对比较 “硬” ，边界受每个邻居点的平均影响。
右图（weights = 'distance'）：因为距离近的邻居点权重更大，决策边界会更倾向于靠近样本点密集的区域，边界形状可能会更曲折，更能适应样本分布的局部特征 

### 本周学习收获
- 通过使用Google Scholar来查找文献，学习了如何使用Google Scholar进行文献搜索，以及如何利用搜索结果来查找相关研究成果。
- 通过使用sklearn的官方数据集实现knn算法，学习了如何使用sklearn中的交叉验证的方式来确定k的取值，以及如何使用KNN算法来进行分类任务。
- 学习了KNN算法的一些参数，以及如何使用KNN算法来进行分类任务。

